<!DOCTYPE html>
<html lang="en">
<head>
          <title>Spacebeam: Distributed Artificial Intelligence</title>
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta charset="utf-8" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Spacebeam: Distributed Artificial Intelligence Full Atom Feed" />
        <link href="/feeds/updates.atom.xml" type="application/atom+xml" rel="alternate" title="Spacebeam: Distributed Artificial Intelligence Categories Atom Feed" />
        <!-- twitter card metadata -->
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="/images/logo.png">
<meta name="twitter:site" content="@SpacebeamOrg">
<meta name="twitter:title" content="Distributed memory">
<meta name="twitter:description" content="The auto-associator learning paradigm, in which the goal is to store specific patterns for future retrieval. Notes from the PDP.17">
        <!-- OG Tags -->
<meta property="og:url" content="/2019/12/28/distributed-memory/"/>
<meta property="og:title" content="Spacebeam: Distributed Artificial Intelligence | Distributed memory" />
<meta property="og:description" content="The auto-associator learning paradigm, in which the goal is to store specific patterns for future retrieval. Notes from the PDP.17" />
        <!-- favicon -->
        <link rel="icon" type="image/png" href="/images/logo.png">
        <!-- moment.js for date formatting -->
        <script src="/theme/js/moment.js"></script>
        <!-- css -->
        <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />
		<script>

                /*! grunt-grunticon Stylesheet Loader - v2.1.2 | https://github.com/filamentgroup/grunticon | (c) 2015 Scott Jehl, Filament Group, Inc. | MIT license. */

    (function(e){function t(t,n,r,o){"use strict";function a(){for(var e,n=0;u.length>n;n++)u[n].href&&u[n].href.indexOf(t)>-1&&(e=!0);e?i.media=r||"all":setTimeout(a)}var i=e.document.createElement("link"),l=n||e.document.getElementsByTagName("script")[0],u=e.document.styleSheets;return i.rel="stylesheet",i.href=t,i.media="only x",i.onload=o||null,l.parentNode.insertBefore(i,l),a(),i}var n=function(r,o){"use strict";if(r&&3===r.length){var a=e.navigator,i=e.Image,l=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===a.userAgent.indexOf("Chrome")||-1!==a.userAgent.indexOf("Series40")),u=new i;u.onerror=function(){n.method="png",n.href=r[2],t(r[2])},u.onload=function(){var e=1===u.width&&1===u.height,a=r[e&&l?0:e?1:2];n.method=e&&l?"svg":e?"datapng":"png",n.href=a,t(a,null,null,o)},u.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};n.loadCSS=t,e.grunticon=n})(this);(function(e,t){"use strict";var n=t.document,r="grunticon:",o=function(e){if(n.attachEvent?"complete"===n.readyState:"loading"!==n.readyState)e();else{var t=!1;n.addEventListener("readystatechange",function(){t||(t=!0,e())},!1)}},a=function(e){return t.document.querySelector('link[href$="'+e+'"]')},c=function(e){var t,n,o,a,c,i,u={};if(t=e.sheet,!t)return u;n=t.cssRules?t.cssRules:t.rules;for(var l=0;n.length>l;l++)o=n[l].cssText,a=r+n[l].selectorText,c=o.split(");")[0].match(/US\-ASCII\,([^"']+)/),c&&c[1]&&(i=decodeURIComponent(c[1]),u[a]=i);return u},i=function(e){var t,o,a;o="data-grunticon-embed";for(var c in e)if(a=c.slice(r.length),t=n.querySelectorAll(a+"["+o+"]"),t.length)for(var i=0;t.length>i;i++)t[i].innerHTML=e[c],t[i].style.backgroundImage="none",t[i].removeAttribute(o);return t},u=function(t){"svg"===e.method&&o(function(){i(c(a(e.href))),"function"==typeof t&&t()})};e.embedIcons=i,e.getCSS=a,e.getIcons=c,e.ready=o,e.svgLoadedCallback=u,e.embedSVG=u})(grunticon,this);

                grunticon(["/theme/css/icons.data.svg.css", "/theme/css/icons.data.png.css", "/theme/css/icons.fallback.css"]);
            </script>
        <noscript><link href="/theme/css/icons.fallback.css" rel="stylesheet"></noscript>
        <!-- menu toggle javascript -->
        <script type="text/javascript">
            document.addEventListener("DOMContentLoaded", initMenu);

            function initMenu(){
                var menu = document.getElementById("menu");
                var menulink = document.getElementById("menu-link");
                menulink.addEventListener("click", function toggleMenu(){
                        window.event.preventDefault();
                        menulink.classList.toggle('active');
                        menu.classList.toggle('active');
                    });
            };
        </script>

    <meta name="description" content="The auto-associator learning paradigm, in which the goal is to store specific patterns for future retrieval. Notes from the PDP.17" />


</head>
<body>
    <div role="banner" id="masthead">
        <header>
            <a href="/"><img src="/images/logo.png" alt="McManus Logo"></a>
            <a href="#menu" id="menu-link">more</a>
            <nav id="menu">
                <ul>
                        <li><a href="/pages/about/">about</a></li>
                        <li><a href="/pages/research/">research</a></li>
                        <li><a href="/pages/technology/">technology</a></li>
                            <li class="active"><a href="/category/updates">updates</a></li>
                        <li class="tag-1">
                        <a href="/tag/guide/">
                        guide
                        </a>
                        </li>
                        <li class="tag-4">
                        <a href="/tag/zergs/">
                        zergs
                        </a>
                        </li>
                </ul>
            </nav>
        </header>
    </div>
        <div class="page" role="main">
  <div class="article" role="article">
    <article>
        <footer>
            <a name="top"></a>
            <p>
              <time datetime=" 2019-12-28 00:00:00+01:00">
                <script>document.write(moment('2019-12-28 00:00:00+01:00').format('LL'));</script>
              </time>
            </p>
        </footer>
        <header>
          <h2>
            Distributed memory
          </h2>
        </header>
      <div class="content">
         <h2>Benefits of Distribution</h2>
<p>Connection information distribution allows us to instruct parallel processing structures from outside the network, making their behavior contingent on instructions originating elsewhere in the network. This means, for example, that the way a network responds to a particular input can be made contingent on the state of some other network in the system, thereby greatly increasing the flexibility of parallel processing mechanisms.</p>
<p>Perhaps the most general way of stating the benefit of connection information distribution is to note that it is analogous, in a way, to the invention of the stored program. The use of centrally stored connection information to program local processing structures is analogous. This allows the very same processing structures to be programmed to perform a very wide range of different tasks.</p>
<h2>A Distributed Model Of Memory</h2>
<p>The auto-associator models are a class of related models that share the auto-associative architecture. That is, they all consist of a single set of units that are completely interconnected, auto-associators are limited by the fact thatthey can only train connections between units whose target activations can be specified from outside the network.</p>
<p>In spite of this limitation, auto-associators have several interesting properties. They can learn to do pattern completion and to restore distorted versions of learned patterns to their original form. They can learn to extract the prototype of a set of patterns from distorted exemplars presented during training.</p>
<p>In all versions of the auto-associator input patterns consist of vectors specifying positive and negative inputs to the
 units from outside the network, based on these external inputs and on the connections they receive from other units inside the network.</p>
<p>Patterns that are scaled by a network are called eigenvectors, eigenvector simply means "same vector." The magnitude of the eigenvector, as it is processed by the network, is called its eigenvalue.</p>
<p>The view that human memory is physiologically distributed within circumscribed regions of the brain seems to be quite a reasonable and plausible assumption.</p>
<p>But given the rather loose coupling between a psychological or cognitive theory of physiological implementation, we can ask, does the notion of distributed memory have anything to offer us in terms of an understanding of human cognition?</p>
<h2>General Properties</h2>
<p>Our model shares a number of basic assumptions about the nature of the processing and memory system with most other distributed models.</p>
<p>In particular, the processing system is assumed to consist of a highly interconnected network of units that take on activation values and communicate with other units by sending signals modulated by weights associated with the connections between the units. Sometimes we may think of the units as corresponding to particular representational primitives, but they need not. For example, even what we might consider to be a primitive feature of something, like having a particular color, might be a pattern of activation over a collection of units.</p>
<h3>Connection information distribution</h3>
<p>We argue that connection information distribution provides a way of overcoming some apparent limitations of parallel distributed processing mechanisms. Using connection information distribution, we can create local copies of relevant portions of the contents of a central knowledge store. These local copies then server as the basis for interactive processing among the conceptual entities they program local hardware units to represent.</p>
<p>With this mechanism, models can now be said to be able to create multiple instantiations of the same schema, bound appropriately to the correct local variables, subject to just the kinds of binding errors humans seem to make, we have not really done anything more than show how existing tools in the arsenal of parallel distributed processing mechanisms can be used to create local copies of networks.</p>
<h3>Modular structure</h3>
<p>We assume that the units are organized into modules. Each module receives inputs from other modules; the units within the module are richly interconnected with each other; and they send outputs to other modules. The state of each module represents a synthesis of the states of all the modules it receives inputs from.</p>
<p>Others will come from relatively more abstract modules, which themselves receive inputs from and send outputs to other modules placed at the abstract end of several different modalities. Thus, each module combines a number of different sources of information.</p>
<h3>Units play specific roles within patterns</h3>
<p>A pattern of activation only counts as the same as another if the same units are involved.</p>
<h2>Relation to Basic Concepts in Memory</h2>
<ol>
<li>
<p><strong>State as pattern of activation</strong>; In a distributed memory system, a mental state is a pattern of activation over the units in some subset of the modules. The patterns in the different modules capture different aspects of the content of the states in a kind of a partially overlapping fashion. Alternative mental states are simply alternative patterns of activations over the modules. Information processing is the process of evolution in time of mental states (?).</p>
</li>
<li>
<p><strong>Memory traces as changes in the weights</strong>; Patterns of activation come and go, leaving traces behind when they have passed. What are the traces? They are changes in the strengths or weights of the connections between the units in the modules. Each memory trace is distributed over many different connections, and each connection participates in many different memory traces. The traces of different mental states are therefore superimposed in the same set of weights.</p>
</li>
<li>
<p><strong>Retrieval as reinstatement of prior pattern of activation</strong>; Retrieval amounts to partial reinstatement of a mental state, using a cue which is a fragment of the original state. For any given module, we can see the cues as originating from outside of it. Some cues could arise ultimately from sensory input. Others would arise from the results of previous retrieval operations, feedback to the memory system under the control of a search or retrieval plan.</p>
</li>
</ol>
      </div>
      <div class="back-to-top">
          <a href="#top">back to top</a>
      </div>
    </article>
  </div>
<!-- end article -->
                <footer>
                    <div class="icons">
                        <a href="https://github.com/spacebeam" target="_blank"><div class="icon-github icon"></div></a>
                        <a href="https://twitter.com/SpacebeamOrg" target="_blank"><div class="icon-twitter icon"></div></a>
                        <a href="https://discord.com/invite/SFpVE5Z" target="_blank"><div class="icon-discord icon"></div></a>
                    </div>
                    <p>© <script>document.write(moment().format('YYYY'));</script> The Spacebeam authors</p>
                </footer>
        </div>
</body>
</html>